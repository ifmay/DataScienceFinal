{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful mysklearn package import statements and reloads\n",
    "import importlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "# uncomment once you paste your myclassifiers.py into mysklearn package\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyKNeighborsClassifier, MyDummyClassifier, MyNaiveBayesClassifier, MyDecisionTreeClassifier, MyRandomForestClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✈️ Flight Delay Classification ✈️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('normalized_flights.csv')\n",
    "\n",
    "# Compute flight delay (in minutes, assuming sched_dep_time and dep_time are timestamps or integers)\n",
    "data['flight_delay'] = data['sched_dep_time'] - data['dep_time']\n",
    "\n",
    "# Extract meaningful features for predicting delays\n",
    "X = data[[\n",
    "    'year', 'month', 'day', 'hour', 'minute',  # Time-based features\n",
    "    'carrier', 'flight', 'tailnum',            # Operational features\n",
    "    'origin', 'dest', 'distance', 'air_time', # Location & flight characteristics\n",
    "    'dep_delay', 'sched_dep_time'             # Existing delay and timing\n",
    "]]\n",
    "\n",
    "# Target variable: flight delay\n",
    "y = data['flight_delay']\n",
    "\n",
    "# Initialize K-Fold cross-validation (10 splits)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Placeholder for performance metrics\n",
    "accuracies, error_rates, precisions, recalls, f1_scores, confusion_matrices = [], [], [], [], [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (assuming `data` is a pandas DataFrame containing the provided sample data)\n",
    "data = pd.read_csv('normalized_flights.csv') \n",
    "\n",
    "# Define delay categories\n",
    "def categorize_delay(delay):\n",
    "    if delay <= 0:\n",
    "        return 0\n",
    "    elif 0 < delay <= 30:\n",
    "        return 1\n",
    "    elif 30 < delay <= 60:\n",
    "        return 2\n",
    "    elif 60 < delay <= 120:\n",
    "        return 3\n",
    "    elif 120 < delay <= 180:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "# Map delays to categories\n",
    "data['delay_category'] = data['arr_delay'].apply(categorize_delay)\n",
    "\n",
    "# Convert categorical features to integers (label encoding)\n",
    "categorical_cols = ['carrier', 'flight', 'tailnum', 'origin', 'dest']  # List your categorical columns\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col].astype(str))  # Convert to string if necessary and apply label encoding\n",
    "    label_encoders[col] = le  # Store the encoder to use it later if needed\n",
    "\n",
    "# Prepare features (X) and labels (y)\n",
    "X = data[['dep_time', 'sched_dep_time', 'dep_delay', 'arr_time', 'sched_arr_time', 'arr_delay', 'air_time', 'distance', 'dest',\n",
    "          'carrier', 'month', 'hour']].fillna(0)\n",
    "\n",
    "y = data['delay_category'].values\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN Performance Evaluation & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X = np.array(X) \n",
    "y = np.array(y) \n",
    "\n",
    "# Initialize StratifiedKFold with k=10 (assuming myevaluation provides this utility)\n",
    "kf = myevaluation.stratified_kfold_split(X, y, n_splits=10, random_state=None, shuffle=False)\n",
    "\n",
    "# Initialize lists to store metrics (accuracy, precision, recall, f1, confusion matrices, etc.)\n",
    "knn_accuracies, knn_precisions, knn_recalls, knn_f1s, knn_conf_matrices = [], [], [], [], []\n",
    "\n",
    "# Initialize lists to gather true and predicted values across all folds\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "# Define the correct labels for the confusion matrix and metrics\n",
    "labels = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, test_index in kf:\n",
    "    knn_X_train, knn_X_test = X[train_index], X[test_index]\n",
    "    knn_y_train, knn_y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Initialize and fit MyKNeighborsClassifier\n",
    "    knn_model = MyKNeighborsClassifier()\n",
    "    knn_model.fit(knn_X_train.tolist(), knn_y_train.tolist())\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    knn_y_pred = knn_model.predict(knn_X_test.tolist())\n",
    "    \n",
    "    # Calculate and store metrics for this fold\n",
    "    accuracy = myevaluation.accuracy_score(knn_y_test, knn_y_pred)\n",
    "    precision_score = myevaluation.binary_precision_score(knn_y_test, knn_y_pred, labels= labels)  # Multi-class precision\n",
    "    recall_score = myevaluation.binary_recall_score(knn_y_test, knn_y_pred, labels= labels)  # Multi-class recall\n",
    "    f1 = myevaluation.binary_f1_score(knn_y_test, knn_y_pred, labels= labels)  # Multi-class F1 score\n",
    "    confusion_matrix = myevaluation.confusion_matrix(knn_y_test, knn_y_pred, labels=labels)\n",
    "\n",
    "    # Store fold metrics\n",
    "    knn_accuracies.append(accuracy)\n",
    "    knn_precisions.append(precision_score)\n",
    "    knn_recalls.append(recall_score)\n",
    "    knn_f1s.append(f1)\n",
    "    knn_conf_matrices.append(confusion_matrix)\n",
    "    \n",
    "    # Collect all true and predicted labels for final confusion matrix\n",
    "    all_y_true.extend(knn_y_test)\n",
    "    all_y_pred.extend(knn_y_pred)\n",
    "\n",
    "# Calculate the final confusion matrix using all folds' predictions\n",
    "final_confusion_matrix = myevaluation.confusion_matrix(all_y_true, all_y_pred, labels=labels)\n",
    "matrix_with_totals = myutils.calculate_confusion_matrix_totals(final_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "kNN Classifier Performance\n",
      "===========================================\n",
      "Accuracy: 0.57\n",
      "Error: 0.43\n",
      "Precision: 0.60\n",
      "Recall: 0.72\n",
      "F1 Score: 0.64\n",
      "\n",
      "Confusion Matrix:\n",
      "  Survived    0    1    2    3    4    5    Total    Recognition %\n",
      "----------  ---  ---  ---  ---  ---  ---  -------  ---------------\n",
      "         0  264   98    4    0    0    3      369            71.54\n",
      "         1  142  123   40    8    0    0      313            39.30\n",
      "         2   37   88   73   29    0    0      227            32.16\n",
      "         3    7   37   55  179   46    0      324            55.25\n",
      "         4    2    1   14   71  175   21      284            61.62\n",
      "         5    0    2    3   10   62  206      283            72.79\n"
     ]
    }
   ],
   "source": [
    "print(\"===========================================\")\n",
    "print(\"kNN Classifier Performance\")\n",
    "print(\"===========================================\")\n",
    "\n",
    "# Calculate and print average performance\n",
    "print(f\"Accuracy: {np.mean(knn_accuracies):.2f}\")\n",
    "print(f\"Error: {1 - np.mean(knn_accuracies):.2f}\")\n",
    "print(f\"Precision: {np.mean(knn_precisions):.2f}\")\n",
    "print(f\"Recall: {np.mean(knn_recalls):.2f}\")\n",
    "print(f\"F1 Score: {np.mean(knn_f1s):.2f}\\n\")\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "myutils.display_confusion_matrix(matrix_with_totals, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Evaluation & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X = np.array(X) \n",
    "y = np.array(y) \n",
    "\n",
    "# Initialize StratifiedKFold with k=10\n",
    "kf = myevaluation.stratified_kfold_split(X, y, n_splits=10, random_state=None, shuffle=False)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "nb_accuracies, nb_precisions, nb_recalls, nb_f1s, nb_conf_matrices = [], [], [], [], []\n",
    "\n",
    "# Initialize lists to collect all true and predicted labels across all folds\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "# Define the correct labels for the confusion matrix and metrics\n",
    "labels = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# Loop over each fold\n",
    "# Loop over each fold in Stratified K-Fold\n",
    "for train_index, test_index in kf:\n",
    "    nb_X_train, nb_X_test = X[train_index], X[test_index]\n",
    "    nb_y_train, nb_y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Initialize and fit MyNaiveBayesClassifier\n",
    "    nb_model = MyNaiveBayesClassifier()\n",
    "    nb_model.fit(nb_X_train.tolist(), nb_y_train.tolist())\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    nb_y_pred = nb_model.predict(nb_X_test.tolist())\n",
    "    \n",
    "    # Calculate and store metrics for this fold\n",
    "    accuracy = myevaluation.accuracy_score(nb_y_test, nb_y_pred)\n",
    "    error = 1 - accuracy\n",
    "    precision_score = myevaluation.binary_precision_score(nb_y_test, nb_y_pred, labels=labels)\n",
    "    recall_score = myevaluation.binary_recall_score(nb_y_test, nb_y_pred, labels=labels)\n",
    "    f1 = myevaluation.binary_f1_score(nb_y_test, nb_y_pred, labels=labels)\n",
    "    confusion_matrix = myevaluation.confusion_matrix(nb_y_test, nb_y_pred, labels=labels)\n",
    "\n",
    "    # Store metrics for this fold\n",
    "    nb_accuracies.append(accuracy)\n",
    "    nb_precisions.append(precision_score)\n",
    "    nb_recalls.append(recall_score)\n",
    "    nb_f1s.append(f1)\n",
    "    nb_conf_matrices.append(confusion_matrix)\n",
    "\n",
    "    # Collect all true and predicted labels for final confusion matrix\n",
    "    all_y_true.extend(nb_y_test)\n",
    "    all_y_pred.extend(nb_y_pred)\n",
    "   \n",
    "\n",
    "# Calculate the final confusion matrix using all folds' predictions\n",
    "final_confusion_matrix = myevaluation.confusion_matrix(all_y_true, all_y_pred, labels=labels)\n",
    "matrix_with_totals = myutils.calculate_confusion_matrix_totals(final_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Naive Bayes Classifier Performance\n",
      "===========================================\n",
      "Accuracy: 0.65\n",
      "Error: 0.35\n",
      "Precision: 0.71\n",
      "Recall: 0.78\n",
      "F1 Score: 0.74\n",
      "\n",
      "Confusion Matrix:\n",
      "  Survived    0    1    2    3    4    5    Total    Recognition %\n",
      "----------  ---  ---  ---  ---  ---  ---  -------  ---------------\n",
      "         0  289   46    9   13    8    4      369            78.32\n",
      "         1   52  207   23   21    4    6      313            66.13\n",
      "         2   18   36  117   39    8    9      227            51.54\n",
      "         3   17   17   18  227   28   17      324            70.06\n",
      "         4   13   12   12   36  181   30      284            63.73\n",
      "         5   29   14   21   30   38  151      283            53.36\n"
     ]
    }
   ],
   "source": [
    "print(\"===========================================\")\n",
    "print(\"Naive Bayes Classifier Performance\")\n",
    "print(\"===========================================\")\n",
    "\n",
    "# Calculate and print average performance\n",
    "print(f\"Accuracy: {np.mean(nb_accuracies):.2f}\")\n",
    "print(f\"Error: {1 - np.mean(nb_accuracies):.2f}\")\n",
    "print(f\"Precision: {np.mean(nb_precisions):.2f}\")\n",
    "print(f\"Recall: {np.mean(nb_recalls):.2f}\")\n",
    "print(f\"F1 Score: {np.mean(nb_f1s):.2f}\\n\")\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "myutils.display_confusion_matrix(matrix_with_totals, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Performance Evaluation & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num trees: 4\n",
      "[['Leaf', 21.0, 1080, 1080], ['Attribute', 'att11', ['Value', 5.0, ['Leaf', 5.0, 2, 1080]], ['Value', 6.0, ['Leaf', 6.0, 16, 1080]], ['Value', 7.0, ['Leaf', 7.0, 26, 1080]], ['Value', 8.0, ['Leaf', 8.0, 31, 1080]], ['Value', 9.0, ['Leaf', 9.0, 33, 1080]], ['Value', 10.0, ['Leaf', 10.0, 44, 1080]], ['Value', 11.0, ['Leaf', 11.0, 37, 1080]], ['Value', 12.0, ['Leaf', 12.0, 37, 1080]], ['Value', 13.0, ['Leaf', 13.0, 29, 1080]], ['Value', 14.0, ['Leaf', 14.0, 51, 1080]], ['Value', 15.0, ['Leaf', 15.0, 47, 1080]], ['Value', 16.0, ['Leaf', 16.0, 76, 1080]], ['Value', 17.0, ['Leaf', 17.0, 81, 1080]], ['Value', 18.0, ['Leaf', 18.0, 54, 1080]], ['Value', 19.0, ['Leaf', 19.0, 117, 1080]], ['Value', 20.0, ['Leaf', 20.0, 83, 1080]], ['Value', 21.0, ['Leaf', 21.0, 80, 1080]], ['Value', 22.0, ['Leaf', 22.0, 65, 1080]], ['Value', 23.0, ['Leaf', 23.0, 69, 1080]]], ['Leaf', 21.0, 1080, 1080], ['Leaf', 21.0, 1080, 1080]]\n",
      "Actual: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Predicted: [21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "21.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Calculate and store metrics for this fold\u001b[39;00m\n\u001b[1;32m     37\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m myevaluation\u001b[38;5;241m.\u001b[39maccuracy_score(rf_y_test, rf_y_pred)\n\u001b[0;32m---> 38\u001b[0m precision_score \u001b[38;5;241m=\u001b[39m myevaluation\u001b[38;5;241m.\u001b[39mbinary_precision_score(rf_y_test, rf_y_pred, labels\u001b[38;5;241m=\u001b[39mlabels)  \u001b[38;5;66;03m# Multi-class precision\u001b[39;00m\n\u001b[1;32m     39\u001b[0m recall_score \u001b[38;5;241m=\u001b[39m myevaluation\u001b[38;5;241m.\u001b[39mbinary_recall_score(rf_y_test, rf_y_pred, labels\u001b[38;5;241m=\u001b[39mlabels)  \u001b[38;5;66;03m# Multi-class recall\u001b[39;00m\n\u001b[1;32m     40\u001b[0m f1 \u001b[38;5;241m=\u001b[39m myevaluation\u001b[38;5;241m.\u001b[39mbinary_f1_score(rf_y_test, rf_y_pred, labels\u001b[38;5;241m=\u001b[39mlabels)  \u001b[38;5;66;03m# Multi-class F1 score\u001b[39;00m\n",
      "File \u001b[0;32m/home/DataScienceFinal/mysklearn/myevaluation.py:399\u001b[0m, in \u001b[0;36mbinary_precision_score\u001b[0;34m(y_true, y_pred, labels, pos_label)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     pos_label \u001b[38;5;241m=\u001b[39m labels[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 399\u001b[0m matrix \u001b[38;5;241m=\u001b[39m confusion_matrix(y_true, y_pred, labels)\n\u001b[1;32m    400\u001b[0m indexed_labels \u001b[38;5;241m=\u001b[39m {label: index \u001b[38;5;28;01mfor\u001b[39;00m index, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(labels)}\n\u001b[1;32m    401\u001b[0m pos_index \u001b[38;5;241m=\u001b[39m indexed_labels[pos_label]\n",
      "File \u001b[0;32m/home/DataScienceFinal/mysklearn/myevaluation.py:231\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m true, pred \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(y_true, y_pred):\n\u001b[1;32m    230\u001b[0m     true_index \u001b[38;5;241m=\u001b[39m label_to_index[true]\n\u001b[0;32m--> 231\u001b[0m     pred_index \u001b[38;5;241m=\u001b[39m label_to_index[pred]\n\u001b[1;32m    232\u001b[0m     matrix[true_index][pred_index] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m matrix\n",
      "\u001b[0;31mKeyError\u001b[0m: 21.0"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Convert data to numpy arrays if not already\n",
    "X = np.array(X) \n",
    "y = np.array(y) \n",
    "\n",
    "# Initialize StratifiedKFold with k=10 (assuming myevaluation provides this utility)\n",
    "kf = myevaluation.stratified_kfold_split(X, y, n_splits=10, random_state=None, shuffle=False)\n",
    "\n",
    "# Initialize lists to store metrics (accuracy, precision, recall, f1, confusion matrices, etc.)\n",
    "rf_accuracies, rf_precisions, rf_recalls, rf_f1s, rf_conf_matrices = [], [], [], [], []\n",
    "\n",
    "# Initialize lists to gather true and predicted values across all folds\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "# Define the correct labels for the confusion matrix and metrics\n",
    "labels = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, test_index in kf:\n",
    "    rf_X_train, rf_X_test = X[train_index], X[test_index]\n",
    "    rf_y_train, rf_y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Initialize and fit RandomForestClassifier\n",
    "    rf_model = MyRandomForestClassifier(n=10, m=4, f=2)  # Adjust hyperparameters as needed\n",
    "    rf_model.fit(rf_X_train, rf_y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    rf_y_pred = rf_model.predict(rf_X_test)\n",
    "\n",
    "    print(f\"Actual: {rf_y_test}\")\n",
    "    print(f\"Predicted: {rf_y_pred}\")\n",
    "    \n",
    "    # Calculate and store metrics for this fold\n",
    "    accuracy = myevaluation.accuracy_score(rf_y_test, rf_y_pred)\n",
    "    precision_score = myevaluation.binary_precision_score(rf_y_test, rf_y_pred, labels=labels)  # Multi-class precision\n",
    "    recall_score = myevaluation.binary_recall_score(rf_y_test, rf_y_pred, labels=labels)  # Multi-class recall\n",
    "    f1 = myevaluation.binary_f1_score(rf_y_test, rf_y_pred, labels=labels)  # Multi-class F1 score\n",
    "    confusion_matrix = myevaluation.confusion_matrix(rf_y_test, rf_y_pred, labels=labels)\n",
    "\n",
    "    # Store fold metrics\n",
    "    rf_accuracies.append(accuracy)\n",
    "    rf_precisions.append(precision_score)\n",
    "    rf_recalls.append(recall_score)\n",
    "    rf_f1s.append(f1)\n",
    "    rf_conf_matrices.append(confusion_matrix)\n",
    "    \n",
    "    # Collect all true and predicted labels for final confusion matrix\n",
    "    all_y_true.extend(rf_y_test)\n",
    "    all_y_pred.extend(rf_y_pred)\n",
    "\n",
    "# Calculate the final confusion matrix using all folds' predictions\n",
    "final_confusion_matrix = myevaluation.confusion_matrix(all_y_true, all_y_pred, labels=labels)\n",
    "matrix_with_totals = myutils.calculate_confusion_matrix_totals(final_confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Random Forest Classifier Performance\n",
      "===========================================\n",
      "Accuracy: 0.98\n",
      "Error: 0.02\n",
      "Precision: 0.99\n",
      "Recall: 1.00\n",
      "F1 Score: 0.99\n",
      "\n",
      "Confusion Matrix:\n",
      "  Survived    0    1    2    3    4    5    Total    Recognition %\n",
      "----------  ---  ---  ---  ---  ---  ---  -------  ---------------\n",
      "         0  369    0    0    0    0    0      369           100.00\n",
      "         1    0  313    0    0    0    0      313           100.00\n",
      "         2    0    4  222    1    0    0      227            97.80\n",
      "         3    0    0   10  311    3    0      324            95.99\n",
      "         4    0    0    0    5  278    1      284            97.89\n",
      "         5    6    0    0    1    2  274      283            96.82\n"
     ]
    }
   ],
   "source": [
    "print(\"===========================================\")\n",
    "print(\"Random Forest Classifier Performance\")\n",
    "print(\"===========================================\")\n",
    "\n",
    "# Calculate and print average performance\n",
    "print(f\"Accuracy: {np.mean(rf_accuracies):.2f}\")\n",
    "print(f\"Error: {1 - np.mean(rf_accuracies):.2f}\")\n",
    "print(f\"Precision: {np.mean(rf_precisions):.2f}\")\n",
    "print(f\"Recall: {np.mean(rf_recalls):.2f}\")\n",
    "print(f\"F1 Score: {np.mean(rf_f1s):.2f}\\n\")\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "myutils.display_confusion_matrix(matrix_with_totals, labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
