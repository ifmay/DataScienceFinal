{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful mysklearn package import statements and reloads\n",
    "import importlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyKNeighborsClassifier, RandomForest, MyDummyClassifier, MyNaiveBayesClassifier, MyDecisionTreeClassifier, MyRandomForestClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✈️ Flight Delay Classification ✈️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('normalized_flights.csv')\n",
    "\n",
    "# Compute flight delay (in minutes, assuming sched_dep_time and dep_time are timestamps or integers)\n",
    "data['flight_delay'] = data['sched_dep_time'] - data['dep_time']\n",
    "\n",
    "# Extract meaningful features for predicting delays\n",
    "X = data[[\n",
    "    'year', 'month', 'day', 'hour', 'minute',  # Time-based features\n",
    "    'carrier', 'flight', 'tailnum',            # Operational features\n",
    "    'origin', 'dest', 'distance', 'air_time', # Location & flight characteristics\n",
    "    'dep_delay', 'sched_dep_time'             # Existing delay and timing\n",
    "]]\n",
    "\n",
    "# Target variable: flight delay\n",
    "y = data['flight_delay']\n",
    "\n",
    "# Initialize K-Fold cross-validation (10 splits)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Placeholder for performance metrics\n",
    "accuracies, error_rates, precisions, recalls, f1_scores, confusion_matrices = [], [], [], [], [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (assuming `data` is a pandas DataFrame containing the provided sample data)\n",
    "data = pd.read_csv('normalized_flights.csv') \n",
    "\n",
    "# Define delay categories\n",
    "def categorize_delay(delay):\n",
    "    if delay <= 0:\n",
    "        return 0\n",
    "    elif 0 < delay <= 30:\n",
    "        return 1\n",
    "    elif 30 < delay <= 60:\n",
    "        return 2\n",
    "    elif 60 < delay <= 120:\n",
    "        return 3\n",
    "    elif 120 < delay <= 180:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "# Map delays to categories\n",
    "data['delay_category'] = data['arr_delay'].apply(categorize_delay)\n",
    "\n",
    "# Convert categorical features to integers (label encoding)\n",
    "categorical_cols = ['carrier', 'flight', 'tailnum', 'origin', 'dest']  # List your categorical columns\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col].astype(str))  # Convert to string if necessary and apply label encoding\n",
    "    label_encoders[col] = le  # Store the encoder to use it later if needed\n",
    "\n",
    "# Prepare features (X) and labels (y)\n",
    "X = data[['dep_time', 'sched_dep_time', 'dep_delay', 'arr_time', 'sched_arr_time', 'arr_delay', 'air_time', 'distance', 'dest',\n",
    "          'carrier', 'month', 'hour']].fillna(0)\n",
    "\n",
    "y = data['delay_category'].values\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN Performance Evaluation & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X = np.array(X) \n",
    "y = np.array(y) \n",
    "\n",
    "# Initialize StratifiedKFold with k=10 (assuming myevaluation provides this utility)\n",
    "kf = myevaluation.stratified_kfold_split(X, y, n_splits=10, random_state=None, shuffle=False)\n",
    "\n",
    "# Initialize lists to store metrics (accuracy, precision, recall, f1, confusion matrices, etc.)\n",
    "knn_accuracies, knn_precisions, knn_recalls, knn_f1s, knn_conf_matrices = [], [], [], [], []\n",
    "\n",
    "# Initialize lists to gather true and predicted values across all folds\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "# Define the correct labels for the confusion matrix and metrics\n",
    "labels = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, test_index in kf:\n",
    "    knn_X_train, knn_X_test = X[train_index], X[test_index]\n",
    "    knn_y_train, knn_y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Initialize and fit MyKNeighborsClassifier\n",
    "    knn_model = MyKNeighborsClassifier()\n",
    "    knn_model.fit(knn_X_train.tolist(), knn_y_train.tolist())\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    knn_y_pred = knn_model.predict(knn_X_test.tolist())\n",
    "    \n",
    "    # Calculate and store metrics for this fold\n",
    "    accuracy = myevaluation.accuracy_score(knn_y_test, knn_y_pred)\n",
    "    precision_score = myevaluation.binary_precision_score(knn_y_test, knn_y_pred, labels= labels)  # Multi-class precision\n",
    "    recall_score = myevaluation.binary_recall_score(knn_y_test, knn_y_pred, labels= labels)  # Multi-class recall\n",
    "    f1 = myevaluation.binary_f1_score(knn_y_test, knn_y_pred, labels= labels)  # Multi-class F1 score\n",
    "    confusion_matrix = myevaluation.confusion_matrix(knn_y_test, knn_y_pred, labels=labels)\n",
    "\n",
    "    # Store fold metrics\n",
    "    knn_accuracies.append(accuracy)\n",
    "    knn_precisions.append(precision_score)\n",
    "    knn_recalls.append(recall_score)\n",
    "    knn_f1s.append(f1)\n",
    "    knn_conf_matrices.append(confusion_matrix)\n",
    "    \n",
    "    # Collect all true and predicted labels for final confusion matrix\n",
    "    all_y_true.extend(knn_y_test)\n",
    "    all_y_pred.extend(knn_y_pred)\n",
    "\n",
    "# Calculate the final confusion matrix using all folds' predictions\n",
    "final_confusion_matrix = myevaluation.confusion_matrix(all_y_true, all_y_pred, labels=labels)\n",
    "matrix_with_totals = myutils.calculate_confusion_matrix_totals(final_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "kNN Classifier Performance\n",
      "===========================================\n",
      "Accuracy: 0.45\n",
      "Error: 0.55\n",
      "Precision: 0.48\n",
      "Recall: 0.74\n",
      "F1 Score: 0.57\n",
      "\n",
      "Confusion Matrix:\n",
      "  Delayed    0    1    2    3    4    5    Total    Recognition %\n",
      "---------  ---  ---  ---  ---  ---  ---  -------  ---------------\n",
      "        0   49    9    6    2    0    0       66            74.24\n",
      "        1   26   14    6    1    1    0       48            29.17\n",
      "        2   14   16    4    1    1    1       37            10.81\n",
      "        3   11    6   10   23    6    1       57            40.35\n",
      "        4    0    1    2   18   20    4       45            44.44\n",
      "        5    1    0    5    4   11   26       47            55.32\n"
     ]
    }
   ],
   "source": [
    "print(\"===========================================\")\n",
    "print(\"kNN Classifier Performance\")\n",
    "print(\"===========================================\")\n",
    "\n",
    "# Calculate and print average performance\n",
    "print(f\"Accuracy: {np.mean(knn_accuracies):.2f}\")\n",
    "print(f\"Error: {1 - np.mean(knn_accuracies):.2f}\")\n",
    "print(f\"Precision: {np.mean(knn_precisions):.2f}\")\n",
    "print(f\"Recall: {np.mean(knn_recalls):.2f}\")\n",
    "print(f\"F1 Score: {np.mean(knn_f1s):.2f}\\n\")\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "myutils.display_confusion_matrix(matrix_with_totals, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Evaluation & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X = np.array(X) \n",
    "y = np.array(y) \n",
    "\n",
    "# Initialize StratifiedKFold with k=10\n",
    "kf = myevaluation.stratified_kfold_split(X, y, n_splits=10, random_state=None, shuffle=False)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "nb_accuracies, nb_precisions, nb_recalls, nb_f1s, nb_conf_matrices = [], [], [], [], []\n",
    "\n",
    "# Initialize lists to collect all true and predicted labels across all folds\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "# Define the correct labels for the confusion matrix and metrics\n",
    "labels = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# Loop over each fold in Stratified K-Fold\n",
    "for train_index, test_index in kf:\n",
    "    nb_X_train, nb_X_test = X[train_index], X[test_index]\n",
    "    nb_y_train, nb_y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Initialize and fit MyNaiveBayesClassifier\n",
    "    nb_model = MyNaiveBayesClassifier()\n",
    "    nb_model.fit(nb_X_train.tolist(), nb_y_train.tolist())\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    nb_y_pred = nb_model.predict(nb_X_test.tolist())\n",
    "    \n",
    "    # Calculate and store metrics for this fold\n",
    "    accuracy = myevaluation.accuracy_score(nb_y_test, nb_y_pred)\n",
    "    error = 1 - accuracy\n",
    "    precision_score = myevaluation.binary_precision_score(nb_y_test, nb_y_pred, labels=labels)\n",
    "    recall_score = myevaluation.binary_recall_score(nb_y_test, nb_y_pred, labels=labels)\n",
    "    f1 = myevaluation.binary_f1_score(nb_y_test, nb_y_pred, labels=labels)\n",
    "    confusion_matrix = myevaluation.confusion_matrix(nb_y_test, nb_y_pred, labels=labels)\n",
    "\n",
    "    # Store metrics for this fold\n",
    "    nb_accuracies.append(accuracy)\n",
    "    nb_precisions.append(precision_score)\n",
    "    nb_recalls.append(recall_score)\n",
    "    nb_f1s.append(f1)\n",
    "    nb_conf_matrices.append(confusion_matrix)\n",
    "\n",
    "    # Collect all true and predicted labels for final confusion matrix\n",
    "    all_y_true.extend(nb_y_test)\n",
    "    all_y_pred.extend(nb_y_pred)\n",
    "   \n",
    "\n",
    "# Calculate the final confusion matrix using all folds' predictions\n",
    "final_confusion_matrix = myevaluation.confusion_matrix(all_y_true, all_y_pred, labels=labels)\n",
    "matrix_with_totals = myutils.calculate_confusion_matrix_totals(final_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Naive Bayes Classifier Performance\n",
      "===========================================\n",
      "Accuracy: 0.37\n",
      "Error: 0.63\n",
      "Precision: 0.50\n",
      "Recall: 0.59\n",
      "F1 Score: 0.53\n",
      "\n",
      "Confusion Matrix:\n",
      "  Delayed    0    1    2    3    4    5    Total    Recognition %\n",
      "---------  ---  ---  ---  ---  ---  ---  -------  ---------------\n",
      "        0   39   10    4    5    4    4       66            59.09\n",
      "        1   12   15    9    9    2    1       48            31.25\n",
      "        2    4   10   14    3    2    4       37            37.84\n",
      "        3    6   10    1   21   10    9       57            36.84\n",
      "        4   10    2    5    9   11    8       45            24.44\n",
      "        5   11    1    5   11    8   11       47            23.40\n"
     ]
    }
   ],
   "source": [
    "print(\"===========================================\")\n",
    "print(\"Naive Bayes Classifier Performance\")\n",
    "print(\"===========================================\")\n",
    "\n",
    "# Calculate and print average performance\n",
    "print(f\"Accuracy: {np.mean(nb_accuracies):.2f}\")\n",
    "print(f\"Error: {1 - np.mean(nb_accuracies):.2f}\")\n",
    "print(f\"Precision: {np.mean(nb_precisions):.2f}\")\n",
    "print(f\"Recall: {np.mean(nb_recalls):.2f}\")\n",
    "print(f\"F1 Score: {np.mean(nb_f1s):.2f}\\n\")\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "myutils.display_confusion_matrix(matrix_with_totals, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Performance Evaluation & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies: [0.7, 0.9666666666666667, 1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 1.0, 0.9333333333333333, 0.8333333333333334]\n",
      "Precisions: [0.7157407407407407, 0.9722222222222222, 1.0, 1.0, 1.0, 1.0, 0.9583333333333334, 1.0, 0.9444444444444445, 0.844047619047619]\n",
      "Recalls: [0.6666666666666666, 0.9583333333333334, 1.0, 1.0, 1.0, 1.0, 0.9166666666666666, 1.0, 0.9249999999999999, 0.8250000000000001]\n",
      "F1 Scores: [0.6726010101010101, 0.961038961038961, 1.0, 1.0, 1.0, 1.0, 0.9285714285714285, 1.0, 0.9273689273689274, 0.826190476190476]\n",
      "Final Confusion Matrix:\n",
      " [[66  0  0  0  0  0]\n",
      " [ 1 47  0  0  0  0]\n",
      " [ 0  2 32  3  0  0]\n",
      " [ 0  0  2 53  2  0]\n",
      " [ 0  0  0  3 39  3]\n",
      " [ 1  0  0  0  2 44]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Initialize StratifiedKFold with k=10\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=False, random_state=None)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "rf_accuracies, rf_precisions, rf_recalls, rf_f1s, rf_conf_matrices = [], [], [], [], []\n",
    "\n",
    "# Initialize lists to collect all true and predicted labels across all folds\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "# Define the correct labels for the confusion matrix and metrics\n",
    "labels = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    rf_X_train, rf_X_test = X[train_index], X[test_index]\n",
    "    rf_y_train, rf_y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Initialize and fit RandomForestClassifier\n",
    "    rf_model = RandomForestClassifier(random_state=42)\n",
    "    rf_model.fit(rf_X_train, rf_y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    rf_y_pred = rf_model.predict(rf_X_test)\n",
    "\n",
    "    # Calculate and store metrics for this fold\n",
    "    accuracy = accuracy_score(rf_y_test, rf_y_pred)\n",
    "    precision = precision_score(rf_y_test, rf_y_pred, labels=labels, average='macro', zero_division=0)\n",
    "    recall = recall_score(rf_y_test, rf_y_pred, labels=labels, average='macro', zero_division=0)\n",
    "    f1 = f1_score(rf_y_test, rf_y_pred, labels=labels, average='macro', zero_division=0)\n",
    "    conf_matrix = confusion_matrix(rf_y_test, rf_y_pred, labels=labels)\n",
    "\n",
    "    # Store metrics for this fold\n",
    "    rf_accuracies.append(accuracy)\n",
    "    rf_precisions.append(precision)\n",
    "    rf_recalls.append(recall)\n",
    "    rf_f1s.append(f1)\n",
    "    rf_conf_matrices.append(conf_matrix)\n",
    "\n",
    "    # Collect all true and predicted labels for final confusion matrix\n",
    "    all_y_true.extend(rf_y_test)\n",
    "    all_y_pred.extend(rf_y_pred)\n",
    "\n",
    "# Calculate the final confusion matrix using all folds' predictions\n",
    "final_confusion_matrix = confusion_matrix(all_y_true, all_y_pred, labels=labels)\n",
    "\n",
    "# Optionally add functionality to calculate totals for the confusion matrix if required\n",
    "# matrix_with_totals = myutils.calculate_confusion_matrix_totals(final_confusion_matrix)\n",
    "\n",
    "# Print or return the results as needed\n",
    "print(\"Accuracies:\", rf_accuracies)\n",
    "print(\"Precisions:\", rf_precisions)\n",
    "print(\"Recalls:\", rf_recalls)\n",
    "print(\"F1 Scores:\", rf_f1s)\n",
    "print(\"Final Confusion Matrix:\\n\", final_confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Random Forest Classifier Performance\n",
      "===========================================\n",
      "Accuracy: 0.94\n",
      "Error: 0.06\n",
      "Precision: 0.94\n",
      "Recall: 0.93\n",
      "F1 Score: 0.93\n",
      "\n",
      "Confusion Matrix:\n",
      "  Delayed    0    1    2    3    4    5    Total    Recognition %\n",
      "---------  ---  ---  ---  ---  ---  ---  -------  ---------------\n",
      "        0   39   10    4    5    4    4       66            59.09\n",
      "        1   12   15    9    9    2    1       48            31.25\n",
      "        2    4   10   14    3    2    4       37            37.84\n",
      "        3    6   10    1   21   10    9       57            36.84\n",
      "        4   10    2    5    9   11    8       45            24.44\n",
      "        5   11    1    5   11    8   11       47            23.40\n"
     ]
    }
   ],
   "source": [
    "print(\"===========================================\")\n",
    "print(\"Random Forest Classifier Performance\")\n",
    "print(\"===========================================\")\n",
    "\n",
    "# Calculate and print average performance\n",
    "print(f\"Accuracy: {np.mean(rf_accuracies):.2f}\")\n",
    "print(f\"Error: {1 - np.mean(rf_accuracies):.2f}\")\n",
    "print(f\"Precision: {np.mean(rf_precisions):.2f}\")\n",
    "print(f\"Recall: {np.mean(rf_recalls):.2f}\")\n",
    "print(f\"F1 Score: {np.mean(rf_f1s):.2f}\\n\")\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "myutils.display_confusion_matrix(matrix_with_totals, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "True Labels: [0 0 0 0 0 0 0 1 1 1 1 1 2 2 2 2 3 3 3 3 3 3 4 4 4 4 4 5 5 5 5 5]\n",
      "Predicted Labels: [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5]\n",
      "Fold 2\n",
      "True Labels: [0 0 0 0 0 0 0 1 1 1 1 1 2 2 2 2 3 3 3 3 3 3 4 4 4 4 4 5 5 5 5 5]\n",
      "Predicted Labels: [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 3, 3, 2, 2, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5]\n",
      "Fold 3\n",
      "True Labels: [0 0 0 0 0 0 0 1 1 1 1 1 2 2 2 2 3 3 3 3 3 3 4 4 4 4 4 5 5 5 5 5]\n",
      "Predicted Labels: [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5]\n",
      "Fold 4\n",
      "True Labels: [0 0 0 0 0 0 0 1 1 1 1 1 2 2 2 2 3 3 3 3 3 3 4 4 4 4 4 5 5 5 5 5]\n",
      "Predicted Labels: [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5]\n",
      "Fold 5\n",
      "True Labels: [0 0 0 0 0 0 0 1 1 1 1 1 2 2 2 2 3 3 3 3 3 3 4 4 4 4 4 5 5 5 5 5]\n",
      "Predicted Labels: [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 4, 5]\n",
      "Fold 6\n",
      "True Labels: [0 0 0 0 0 0 0 1 1 1 1 1 2 2 2 2 3 3 3 3 3 3 4 4 4 4 5 5 5 5 5]\n",
      "Predicted Labels: [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 0, 5, 5]\n",
      "Fold 7\n",
      "True Labels: [0 0 0 0 0 0 1 1 1 1 1 2 2 2 2 3 3 3 3 3 3 4 4 4 4 5 5 5 5 5]\n",
      "Predicted Labels: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5]\n",
      "Fold 8\n",
      "True Labels: [0 0 0 0 0 0 1 1 1 1 1 2 2 2 3 3 3 3 3 4 4 4 4 5 5 5 5]\n",
      "Predicted Labels: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 3, 5, 5, 5, 5]\n",
      "Fold 9\n",
      "True Labels: [0 0 0 0 0 0 1 1 1 1 2 2 2 3 3 3 3 3 4 4 4 4 5 5 5 5]\n",
      "Predicted Labels: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 4, 0]\n",
      "Fold 10\n",
      "True Labels: [0 0 0 0 0 0 1 1 1 1 2 2 2 3 3 3 3 3 4 4 4 4 5 5 5 5]\n",
      "Predicted Labels: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5]\n",
      "Accuracies: [1.0, 0.90625, 1.0, 1.0, 0.96875, 0.967741935483871, 0.9666666666666667, 0.9259259259259259, 0.9230769230769231, 1.0]\n",
      "Precisions: [1.0, 1.0, 1.0, 1.0, 1.0, 0.875, 1.0, 0.8571428571428571, 0.8571428571428571, 1.0]\n",
      "Recalls: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "F1 Scores: [1.0, 1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 1.0, 0.923076923076923, 0.923076923076923, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Convert features and labels to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Perform manual stratified K-fold\n",
    "n_splits = 10\n",
    "indices = np.arange(len(y))\n",
    "unique_classes, y_counts = np.unique(y, return_counts=True)\n",
    "folds = {i: [] for i in range(n_splits)}\n",
    "\n",
    "for cls in unique_classes:\n",
    "    cls_indices = indices[y == cls]\n",
    "    np.random.shuffle(cls_indices)\n",
    "    for i, index in enumerate(cls_indices):\n",
    "        folds[i % n_splits].append(index)\n",
    "\n",
    "fold_indices = [np.array(folds[i]) for i in range(n_splits)]\n",
    "\n",
    "# Metrics storage\n",
    "rf_accuracies, rf_precisions, rf_recalls, rf_f1s = [], [], [], []\n",
    "\n",
    "# Iterate through folds\n",
    "for i in range(n_splits):\n",
    "    test_indices = fold_indices[i]\n",
    "    train_indices = np.concatenate([fold_indices[j] for j in range(n_splits) if j != i])\n",
    "\n",
    "    rf_X_train, rf_X_test = X[train_indices], X[test_indices]\n",
    "    rf_y_train, rf_y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "    # Train Random Forest\n",
    "    rf_model = MyRandomForestClassifier(n_estimators=10, max_depth=5)\n",
    "    rf_model.fit(rf_X_train, rf_y_train)\n",
    "\n",
    "    # Predictions\n",
    "    rf_y_pred = rf_model.predict(rf_X_test)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = np.sum(rf_y_test == rf_y_pred) / len(rf_y_test)\n",
    "    precision = myevaluation.binary_precision_score(rf_y_test, rf_y_pred)\n",
    "    recall = myevaluation.binary_recall_score(rf_y_test, rf_y_pred)\n",
    "    f1 = myevaluation.binary_f1_score(rf_y_test, rf_y_pred)\n",
    "\n",
    "    # Store results\n",
    "    rf_accuracies.append(accuracy)\n",
    "    rf_precisions.append(precision)\n",
    "    rf_recalls.append(recall)\n",
    "    rf_f1s.append(f1)\n",
    "\n",
    "    # Debugging outputs\n",
    "    print(f\"Fold {i + 1}\")\n",
    "    print(\"True Labels:\", rf_y_test)\n",
    "    print(\"Predicted Labels:\", rf_y_pred)\n",
    "\n",
    "# Print final metrics\n",
    "print(\"Accuracies:\", rf_accuracies)\n",
    "print(\"Precisions:\", rf_precisions)\n",
    "print(\"Recalls:\", rf_recalls)\n",
    "print(\"F1 Scores:\", rf_f1s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Random Forest Classifier Performance\n",
      "===========================================\n",
      "Accuracy: 0.97\n",
      "Error: 0.03\n",
      "Precision: 0.96\n",
      "Recall: 1.00\n",
      "F1 Score: 0.98\n",
      "\n",
      "Confusion Matrix:\n",
      "  Delayed    0    1    2    3    4    5    Total    Recognition %\n",
      "---------  ---  ---  ---  ---  ---  ---  -------  ---------------\n",
      "        0   39   10    4    5    4    4       66            59.09\n",
      "        1   12   15    9    9    2    1       48            31.25\n",
      "        2    4   10   14    3    2    4       37            37.84\n",
      "        3    6   10    1   21   10    9       57            36.84\n",
      "        4   10    2    5    9   11    8       45            24.44\n",
      "        5   11    1    5   11    8   11       47            23.40\n"
     ]
    }
   ],
   "source": [
    "print(\"===========================================\")\n",
    "print(\"Random Forest Classifier Performance\")\n",
    "print(\"===========================================\")\n",
    "\n",
    "# Calculate and print average performance\n",
    "print(f\"Accuracy: {np.mean(rf_accuracies):.2f}\")\n",
    "print(f\"Error: {1 - np.mean(rf_accuracies):.2f}\")\n",
    "print(f\"Precision: {np.mean(rf_precisions):.2f}\")\n",
    "print(f\"Recall: {np.mean(rf_recalls):.2f}\")\n",
    "print(f\"F1 Score: {np.mean(rf_f1s):.2f}\\n\")\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "myutils.display_confusion_matrix(matrix_with_totals, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
