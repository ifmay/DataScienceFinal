{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful mysklearn package import statements and reloads\n",
    "import importlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "# uncomment once you paste your myclassifiers.py into mysklearn package\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyKNeighborsClassifier, MyDummyClassifier, MyNaiveBayesClassifier, MyDecisionTreeClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('normalized_flights.csv')\n",
    "\n",
    "# Compute flight delay (in minutes, assuming sched_dep_time and dep_time are timestamps or integers)\n",
    "data['flight_delay'] = data['sched_dep_time'] - data['dep_time']\n",
    "\n",
    "# Extract meaningful features for predicting delays\n",
    "X = data[[\n",
    "    'year', 'month', 'day', 'hour', 'minute',  # Time-based features\n",
    "    'carrier', 'flight', 'tailnum',            # Operational features\n",
    "    'origin', 'dest', 'distance', 'air_time', # Location & flight characteristics\n",
    "    'dep_delay', 'sched_dep_time'             # Existing delay and timing\n",
    "]]\n",
    "\n",
    "# Target variable: flight delay\n",
    "y = data['flight_delay']\n",
    "\n",
    "# Initialize K-Fold cross-validation (10 splits)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Placeholder for performance metrics\n",
    "accuracies, error_rates, precisions, recalls, f1_scores, confusion_matrices = [], [], [], [], [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (assuming `data` is a pandas DataFrame containing the provided sample data)\n",
    "data = pd.read_csv('normalized_flights.csv') \n",
    "\n",
    "# Define delay categories\n",
    "def categorize_delay(delay):\n",
    "    if delay <= 0:\n",
    "        return 0\n",
    "    elif 0 < delay <= 30:\n",
    "        return 1\n",
    "    elif 30 < delay <= 60:\n",
    "        return 2\n",
    "    elif 60 < delay <= 120:\n",
    "        return 3\n",
    "    elif 120 < delay <= 180:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "# Map delays to categories\n",
    "data['delay_category'] = data['arr_delay'].apply(categorize_delay)\n",
    "\n",
    "# Prepare features (X) and labels (y)\n",
    "X = data[['dep_time', 'sched_dep_time', 'dep_delay', 'air_time', 'distance']].fillna(0).values\n",
    "y = data['delay_category'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X) \n",
    "y = np.array(y) \n",
    "\n",
    "# Initialize StratifiedKFold with k=10 (assuming myevaluation provides this utility)\n",
    "kf = myevaluation.stratified_kfold_split(X, y, n_splits=10, random_state=None, shuffle=False)\n",
    "\n",
    "# Initialize lists to store metrics (accuracy, precision, recall, f1, confusion matrices, etc.)\n",
    "knn_accuracies, knn_precisions, knn_recalls, knn_f1s, knn_conf_matrices = [], [], [], [], []\n",
    "dummy_accuracies, dummy_precisions, dummy_recalls, dummy_f1s, dummy_conf_matrices = [], [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to gather true and predicted values across all folds\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, test_index in kf:\n",
    "    knn_X_train, knn_X_test = X[train_index], X[test_index]\n",
    "    knn_y_train, knn_y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Initialize and fit MyKNeighborsClassifier\n",
    "    knn_model = MyKNeighborsClassifier()\n",
    "    knn_model.fit(knn_X_train.tolist(), knn_y_train.tolist())\n",
    "    \n",
    "    # Make predictions\n",
    "    knn_y_pred = knn_model.predict(knn_X_test.tolist())\n",
    "    \n",
    "    # Calculate and store metrics for this fold\n",
    "    accuracy = myevaluation.accuracy_score(knn_y_test, knn_y_pred)\n",
    "    error = 1 - accuracy\n",
    "    precision_score = myevaluation.binary_precision_score(knn_y_test, knn_y_pred, pos_label=\"yes\")\n",
    "    recall_score = myevaluation.binary_recall_score(knn_y_test, knn_y_pred, pos_label=\"yes\")\n",
    "    f1 = myevaluation.binary_f1_score(knn_y_test, knn_y_pred, pos_label=\"yes\")\n",
    "    confusion_matrix = myevaluation.confusion_matrix(knn_y_test, knn_y_pred, labels=[\"yes\", \"no\"])\n",
    "\n",
    "    # Store fold metrics\n",
    "    knn_accuracies.append(accuracy)\n",
    "    knn_precisions.append(precision_score)\n",
    "    knn_recalls.append(recall_score)\n",
    "    knn_f1s.append(f1)\n",
    "    knn_conf_matrices.append(confusion_matrix)\n",
    "\n",
    "    # Collect all true and predicted labels for final confusion matrix\n",
    "    all_y_true.extend(knn_y_test)\n",
    "    all_y_pred.extend(knn_y_pred)\n",
    "\n",
    "# Calculate the final confusion matrix using all folds' predictions\n",
    "labels = [0, 1, 2, 3, 4, 5]\n",
    "final_confusion_matrix = myevaluation.confusion_matrix(all_y_true, all_y_pred, labels=labels)\n",
    "matrix_with_totals = myutils.calculate_confusion_matrix_totals(final_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### kNN Performance Evaluation & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "kNN Classifier Performance\n",
      "===========================================\n",
      "Accuracy: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Calculate and print average performance\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(knn_accuracies)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(knn_precisions)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(knn_recalls)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'error' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"===========================================\")\n",
    "print(\"kNN Classifier Performance\")\n",
    "print(\"===========================================\")\n",
    "\n",
    "# Calculate and print average performance\n",
    "print(f\"Accuracy: {np.mean(knn_accuracies):.2f}\")\n",
    "print(f\"Error: {error:.2f}\")\n",
    "print(f\"Precision: {np.mean(knn_precisions):.2f}\")\n",
    "print(f\"Recall: {np.mean(knn_recalls):.2f}\")\n",
    "print(f\"F1 Score: {np.mean(knn_f1s):.2f}\\n\")\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "myutils.display_confusion_matrix(matrix_with_totals, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MyDummyClassifier.__init__() got an unexpected keyword argument 'strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m dummy_y_train, dummy_y_test \u001b[38;5;241m=\u001b[39m y[train_index], y[test_index]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Initialize and fit MyDummyClassifier\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m dummy_model \u001b[38;5;241m=\u001b[39m MyDummyClassifier(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmost_frequent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m dummy_model\u001b[38;5;241m.\u001b[39mfit(dummy_y_train\u001b[38;5;241m.\u001b[39mtolist())  \u001b[38;5;66;03m# Fit the model on the training labels only\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Make predictions on the test set\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: MyDummyClassifier.__init__() got an unexpected keyword argument 'strategy'"
     ]
    }
   ],
   "source": [
    "# Convert X and y to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Initialize StratifiedKFold with k=10\n",
    "kf = myevaluation.stratified_kfold_split(X, y, n_splits=10, random_state=None, shuffle=False)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "dummy_accuracies, dummy_precisions, dummy_recalls, dummy_f1s, dummy_conf_matrices = [], [], [], [], []\n",
    "\n",
    "# Initialize lists to collect all true and predicted labels across all folds\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "# Loop over each fold in Stratified K-Fold\n",
    "for train_index, test_index in kf:\n",
    "    dummy_X_train, dummy_X_test = X[train_index], X[test_index]\n",
    "    dummy_y_train, dummy_y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Initialize and fit MyDummyClassifier\n",
    "    dummy_model = MyDummyClassifier(strategy=\"most_frequent\")\n",
    "    dummy_model.fit(dummy_y_train.tolist())  # Fit the model on the training labels only\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    dummy_y_pred = dummy_model.predict(dummy_X_test.tolist())\n",
    "    \n",
    "    # Calculate and store metrics for this fold\n",
    "    accuracy = myevaluation.accuracy_score(dummy_y_test, dummy_y_pred)\n",
    "    error = 1 - accuracy\n",
    "    precision_score = myevaluation.binary_precision_score(dummy_y_test, dummy_y_pred, pos_label=\"yes\")\n",
    "    recall_score = myevaluation.binary_recall_score(dummy_y_test, dummy_y_pred, pos_label=\"yes\")\n",
    "    f1 = myevaluation.binary_f1_score(dummy_y_test, dummy_y_pred, pos_label=\"yes\")\n",
    "    confusion_matrix = myevaluation.confusion_matrix(dummy_y_test, dummy_y_pred, labels=[\"yes\", \"no\"])\n",
    "\n",
    "    # Store metrics for this fold\n",
    "    dummy_accuracies.append(accuracy)\n",
    "    dummy_precisions.append(precision_score)\n",
    "    dummy_recalls.append(recall_score)\n",
    "    dummy_f1s.append(f1)\n",
    "    dummy_conf_matrices.append(confusion_matrix)\n",
    "\n",
    "    # Collect all true and predicted labels for final confusion matrix\n",
    "    all_y_true.extend(dummy_y_test)\n",
    "    all_y_pred.extend(dummy_y_pred)\n",
    "\n",
    "# Calculate the final confusion matrix using all folds' predictions\n",
    "labels = [\"yes\", \"no\"]\n",
    "final_confusion_matrix = myevaluation.confusion_matrix(all_y_true, all_y_pred, labels=labels)\n",
    "matrix_with_totals = myutils.calculate_confusion_matrix_totals(final_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Performance Evaluation & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===========================================\")\n",
    "print(\"Dummy Classifier Performance\")\n",
    "print(\"===========================================\")\n",
    "\n",
    "# Calculate and print average performance\n",
    "print(f\"Accuracy: {np.mean(dummy_accuracies):.2f}\")\n",
    "print(f\"Error: {error:.2f}\")\n",
    "print(f\"Precision: {np.mean(dummy_precisions):.2f}\")\n",
    "print(f\"Recall: {np.mean(dummy_recalls):.2f}\")\n",
    "print(f\"F1 Score: {np.mean(dummy_f1s):.2f}\\n\")\n",
    "\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "myutils.display_confusion_matrix(matrix_with_totals, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m recall_score \u001b[38;5;241m=\u001b[39m myevaluation\u001b[38;5;241m.\u001b[39mbinary_recall_score(nb_y_test, nb_y_pred, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m f1 \u001b[38;5;241m=\u001b[39m myevaluation\u001b[38;5;241m.\u001b[39mbinary_f1_score(nb_y_test, nb_y_pred, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m confusion_matrix \u001b[38;5;241m=\u001b[39m myevaluation\u001b[38;5;241m.\u001b[39mconfusion_matrix(nb_y_test, nb_y_pred, labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Store metrics for this fold\u001b[39;00m\n\u001b[1;32m     36\u001b[0m nb_accuracies\u001b[38;5;241m.\u001b[39mappend(accuracy)\n",
      "File \u001b[0;32m/home/DataScienceFinal/mysklearn/myevaluation.py:251\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Populate the confusion matrix\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m true, pred \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(y_true, y_pred):\n\u001b[0;32m--> 251\u001b[0m     true_index \u001b[38;5;241m=\u001b[39m label_to_index[true]\n\u001b[1;32m    252\u001b[0m     pred_index \u001b[38;5;241m=\u001b[39m label_to_index[pred]\n\u001b[1;32m    253\u001b[0m     matrix[true_index][pred_index] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Convert X and y to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Initialize StratifiedKFold with k=10\n",
    "kf = myevaluation.stratified_kfold_split(X, y, n_splits=10, random_state=None, shuffle=False)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "nb_accuracies, nb_precisions, nb_recalls, nb_f1s, nb_conf_matrices = [], [], [], [], []\n",
    "\n",
    "# Initialize lists to collect all true and predicted labels across all folds\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "# Loop over each fold in Stratified K-Fold\n",
    "for train_index, test_index in kf:\n",
    "    nb_X_train, nb_X_test = X[train_index], X[test_index]\n",
    "    nb_y_train, nb_y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Initialize and fit MyNaiveBayesClassifier\n",
    "    nb_model = MyNaiveBayesClassifier()\n",
    "    nb_model.fit(nb_X_train.tolist(), nb_y_train.tolist())\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    nb_y_pred = nb_model.predict(nb_X_test.tolist())\n",
    "    \n",
    "    # Calculate and store metrics for this fold\n",
    "    accuracy = myevaluation.accuracy_score(nb_y_test, nb_y_pred)\n",
    "    error = 1 - accuracy\n",
    "    precision_score = myevaluation.binary_precision_score(nb_y_test, nb_y_pred, pos_label=\"yes\")\n",
    "    recall_score = myevaluation.binary_recall_score(nb_y_test, nb_y_pred, pos_label=\"yes\")\n",
    "    f1 = myevaluation.binary_f1_score(nb_y_test, nb_y_pred, pos_label=\"yes\")\n",
    "    confusion_matrix = myevaluation.confusion_matrix(nb_y_test, nb_y_pred, labels=[\"yes\", \"no\"])\n",
    "\n",
    "    # Store metrics for this fold\n",
    "    nb_accuracies.append(accuracy)\n",
    "    nb_precisions.append(precision_score)\n",
    "    nb_recalls.append(recall_score)\n",
    "    nb_f1s.append(f1)\n",
    "    nb_conf_matrices.append(confusion_matrix)\n",
    "\n",
    "    # Collect all true and predicted labels for final confusion matrix\n",
    "    all_y_true.extend(nb_y_test)\n",
    "    all_y_pred.extend(nb_y_pred)\n",
    "\n",
    "# Calculate the final confusion matrix using all folds' predictions\n",
    "labels = [0, 1, 2, 3, 4, 5]\n",
    "final_confusion_matrix = myevaluation.confusion_matrix(all_y_true, all_y_pred, labels=labels)\n",
    "matrix_with_totals = myutils.calculate_confusion_matrix_totals(final_confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
