{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful mysklearn package import statements and reloads\n",
    "import importlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyKNeighborsClassifier, RandomForest, MyDummyClassifier, MyNaiveBayesClassifier, MyDecisionTreeClassifier, MyRandomForestClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✈️ Flight Delay Classification ✈️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('balanced_flights.csv')\n",
    "\n",
    "# Compute flight delay (in minutes, assuming sched_dep_time and dep_time are timestamps or integers)\n",
    "data['flight_delay'] = data['sched_dep_time'] - data['dep_time']\n",
    "\n",
    "# Extract meaningful features for predicting delays\n",
    "X = data[[\n",
    "    'year', 'month', 'day', 'hour', 'minute',  # Time-based features\n",
    "    'carrier', 'flight', 'tailnum',            # Operational features\n",
    "    'origin', 'dest', 'distance', 'air_time', # Location & flight characteristics\n",
    "    'dep_delay', 'sched_dep_time'             # Existing delay and timing\n",
    "]]\n",
    "\n",
    "# Target variable: flight delay\n",
    "y = data['flight_delay']\n",
    "\n",
    "# Initialize K-Fold cross-validation (10 splits)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Placeholder for performance metrics\n",
    "accuracies, error_rates, precisions, recalls, f1_scores, confusion_matrices = [], [], [], [], [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (assuming `data` is a pandas DataFrame containing the provided sample data)\n",
    "data = pd.read_csv('balanced_flights.csv') \n",
    "\n",
    "# Define delay categories\n",
    "def categorize_delay(delay):\n",
    "    if delay <= 0:\n",
    "        return 0\n",
    "    elif 0 < delay <= 30:\n",
    "        return 1\n",
    "    elif 30 < delay <= 60:\n",
    "        return 2\n",
    "    elif 60 < delay <= 120:\n",
    "        return 3\n",
    "    elif 120 < delay <= 180:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "# Map delays to categories\n",
    "data['delay_category'] = data['dep_delay'].apply(categorize_delay)\n",
    "\n",
    "# Convert categorical features to integers (label encoding)\n",
    "categorical_cols = ['carrier', 'flight', 'tailnum', 'origin', 'dest']  # List your categorical columns\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col].astype(str))  # Convert to string if necessary and apply label encoding\n",
    "    label_encoders[col] = le  # Store the encoder to use it later if needed\n",
    "\n",
    "# Prepare features (X) and labels (y)\n",
    "X = data[['dep_time', 'sched_dep_time', 'sched_arr_time', 'air_time', 'dest',\n",
    "          'carrier', 'hour']].fillna(0)\n",
    "\n",
    "y = data['delay_category'].values\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN Performance Evaluation & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X = np.array(X) \n",
    "y = np.array(y) \n",
    "\n",
    "# Initialize StratifiedKFold with k=10 (assuming myevaluation provides this utility)\n",
    "kf = myevaluation.stratified_kfold_split(X, y, n_splits=10, random_state=None, shuffle=False)\n",
    "\n",
    "# Initialize lists to store metrics (accuracy, precision, recall, f1, confusion matrices, etc.)\n",
    "knn_accuracies, knn_precisions, knn_recalls, knn_f1s, knn_conf_matrices = [], [], [], [], []\n",
    "\n",
    "# Initialize lists to gather true and predicted values across all folds\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "# Define the correct labels for the confusion matrix and metrics\n",
    "labels = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, test_index in kf:\n",
    "    knn_X_train, knn_X_test = X[train_index], X[test_index]\n",
    "    knn_y_train, knn_y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Initialize and fit MyKNeighborsClassifier\n",
    "    knn_model = MyKNeighborsClassifier()\n",
    "    knn_model.fit(knn_X_train.tolist(), knn_y_train.tolist())\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    knn_y_pred = knn_model.predict(knn_X_test.tolist())\n",
    "    \n",
    "    # Calculate and store metrics for this fold\n",
    "    accuracy = myevaluation.accuracy_score(knn_y_test, knn_y_pred)\n",
    "    precision_score = myevaluation.binary_precision_score(knn_y_test, knn_y_pred, labels= labels)  # Multi-class precision\n",
    "    recall_score = myevaluation.binary_recall_score(knn_y_test, knn_y_pred, labels= labels)  # Multi-class recall\n",
    "    f1 = myevaluation.binary_f1_score(knn_y_test, knn_y_pred, labels= labels)  # Multi-class F1 score\n",
    "    confusion_matrix = myevaluation.confusion_matrix(knn_y_test, knn_y_pred, labels=labels)\n",
    "\n",
    "    # Store fold metrics\n",
    "    knn_accuracies.append(accuracy)\n",
    "    knn_precisions.append(precision_score)\n",
    "    knn_recalls.append(recall_score)\n",
    "    knn_f1s.append(f1)\n",
    "    knn_conf_matrices.append(confusion_matrix)\n",
    "    \n",
    "    # Collect all true and predicted labels for final confusion matrix\n",
    "    all_y_true.extend(knn_y_test)\n",
    "    all_y_pred.extend(knn_y_pred)\n",
    "\n",
    "# Calculate the final confusion matrix using all folds' predictions\n",
    "final_confusion_matrix = myevaluation.confusion_matrix(all_y_true, all_y_pred, labels=labels)\n",
    "matrix_with_totals = myutils.calculate_confusion_matrix_totals(final_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "kNN Classifier Performance\n",
      "===========================================\n",
      "Accuracy: 0.59\n",
      "Error: 0.41\n",
      "Precision: 0.52\n",
      "Recall: 0.66\n",
      "F1 Score: 0.58\n",
      "\n",
      "Confusion Matrix:\n",
      "  Delayed    0    1    2    3    4    5    Total    Recognition %\n",
      "---------  ---  ---  ---  ---  ---  ---  -------  ---------------\n",
      "        0  199   83   16    0    0    2      300            66.33\n",
      "        1  125  115   48    8    2    2      300            38.33\n",
      "        2   50   73  130   43    4    0      300            43.33\n",
      "        3    6   20   68  171   35    0      300            57.00\n",
      "        4    0    3   10   54  206   27      300            68.67\n",
      "        5    0    0    4   10   48  238      300            79.33\n"
     ]
    }
   ],
   "source": [
    "print(\"===========================================\")\n",
    "print(\"kNN Classifier Performance\")\n",
    "print(\"===========================================\")\n",
    "\n",
    "# Calculate and print average performance\n",
    "print(f\"Accuracy: {np.mean(knn_accuracies):.2f}\")\n",
    "print(f\"Error: {1 - np.mean(knn_accuracies):.2f}\")\n",
    "print(f\"Precision: {np.mean(knn_precisions):.2f}\")\n",
    "print(f\"Recall: {np.mean(knn_recalls):.2f}\")\n",
    "print(f\"F1 Score: {np.mean(knn_f1s):.2f}\\n\")\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "myutils.display_confusion_matrix(matrix_with_totals, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Evaluation & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X = np.array(X) \n",
    "y = np.array(y) \n",
    "\n",
    "# Initialize StratifiedKFold with k=10\n",
    "kf = myevaluation.stratified_kfold_split(X, y, n_splits=10, random_state=None, shuffle=False)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "nb_accuracies, nb_precisions, nb_recalls, nb_f1s, nb_conf_matrices = [], [], [], [], []\n",
    "\n",
    "# Initialize lists to collect all true and predicted labels across all folds\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "# Define the correct labels for the confusion matrix and metrics\n",
    "labels = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# Loop over each fold in Stratified K-Fold\n",
    "for train_index, test_index in kf:\n",
    "    nb_X_train, nb_X_test = X[train_index], X[test_index]\n",
    "    nb_y_train, nb_y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Initialize and fit MyNaiveBayesClassifier\n",
    "    nb_model = MyNaiveBayesClassifier()\n",
    "    nb_model.fit(nb_X_train.tolist(), nb_y_train.tolist())\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    nb_y_pred = nb_model.predict(nb_X_test.tolist())\n",
    "    \n",
    "    # Calculate and store metrics for this fold\n",
    "    accuracy = myevaluation.accuracy_score(nb_y_test, nb_y_pred)\n",
    "    error = 1 - accuracy\n",
    "    precision_score = myevaluation.binary_precision_score(nb_y_test, nb_y_pred, labels=labels)\n",
    "    recall_score = myevaluation.binary_recall_score(nb_y_test, nb_y_pred, labels=labels)\n",
    "    f1 = myevaluation.binary_f1_score(nb_y_test, nb_y_pred, labels=labels)\n",
    "    confusion_matrix = myevaluation.confusion_matrix(nb_y_test, nb_y_pred, labels=labels)\n",
    "\n",
    "    # Store metrics for this fold\n",
    "    nb_accuracies.append(accuracy)\n",
    "    nb_precisions.append(precision_score)\n",
    "    nb_recalls.append(recall_score)\n",
    "    nb_f1s.append(f1)\n",
    "    nb_conf_matrices.append(confusion_matrix)\n",
    "\n",
    "    # Collect all true and predicted labels for final confusion matrix\n",
    "    all_y_true.extend(nb_y_test)\n",
    "    all_y_pred.extend(nb_y_pred)\n",
    "   \n",
    "\n",
    "# Calculate the final confusion matrix using all folds' predictions\n",
    "final_confusion_matrix = myevaluation.confusion_matrix(all_y_true, all_y_pred, labels=labels)\n",
    "matrix_with_totals = myutils.calculate_confusion_matrix_totals(final_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Naive Bayes Classifier Performance\n",
      "===========================================\n",
      "Accuracy: 0.66\n",
      "Error: 0.34\n",
      "Precision: 0.72\n",
      "Recall: 0.80\n",
      "F1 Score: 0.75\n",
      "\n",
      "Confusion Matrix:\n",
      "  Delayed    0    1    2    3    4    5    Total    Recognition %\n",
      "---------  ---  ---  ---  ---  ---  ---  -------  ---------------\n",
      "        0  295   43    9   11    7    4      369            79.95\n",
      "        1   49  213   21   21    4    5      313            68.05\n",
      "        2   20   34  120   35    9    9      227            52.86\n",
      "        3   15   16   20  227   27   19      324            70.06\n",
      "        4   13   13   10   35  185   28      284            65.14\n",
      "        5   28   13   21   29   40  152      283            53.71\n"
     ]
    }
   ],
   "source": [
    "print(\"===========================================\")\n",
    "print(\"Naive Bayes Classifier Performance\")\n",
    "print(\"===========================================\")\n",
    "\n",
    "# Calculate and print average performance\n",
    "print(f\"Accuracy: {np.mean(nb_accuracies):.2f}\")\n",
    "print(f\"Error: {1 - np.mean(nb_accuracies):.2f}\")\n",
    "print(f\"Precision: {np.mean(nb_precisions):.2f}\")\n",
    "print(f\"Recall: {np.mean(nb_recalls):.2f}\")\n",
    "print(f\"F1 Score: {np.mean(nb_f1s):.2f}\\n\")\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "myutils.display_confusion_matrix(matrix_with_totals, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Performance Evaluation & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies: [0.9055555555555556, 0.9888888888888889, 1.0, 1.0, 1.0, 1.0, 0.9944444444444445, 1.0, 0.9944444444444445, 0.9944444444444445]\n",
      "Precisions: [0.9123146326034005, 0.9877031181379007, 1.0, 1.0, 1.0, 1.0, 0.9930555555555557, 1.0, 0.9944444444444445, 0.9942528735632185]\n",
      "Recalls: [0.8975468975468975, 0.988997113997114, 1.0, 1.0, 1.0, 1.0, 0.9946236559139785, 1.0, 0.9947916666666666, 0.9942528735632185]\n",
      "F1 Scores: [0.8992764656367598, 0.9882154882154882, 1.0, 1.0, 1.0, 1.0, 0.9937216602720614, 1.0, 0.9945296385974353, 0.9941520467836256]\n",
      "Final Confusion Matrix:\n",
      " [[369   0   0   0   0   0]\n",
      " [  0 312   1   0   0   0]\n",
      " [  0   3 224   0   0   0]\n",
      " [  0   1   4 318   1   0]\n",
      " [  0   0   0   3 280   1]\n",
      " [  6   0   1   0   1 275]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Initialize StratifiedKFold with k=10\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=False, random_state=None)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "rf_accuracies, rf_precisions, rf_recalls, rf_f1s, rf_conf_matrices = [], [], [], [], []\n",
    "\n",
    "# Initialize lists to collect all true and predicted labels across all folds\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "# Define the correct labels for the confusion matrix and metrics\n",
    "labels = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    rf_X_train, rf_X_test = X[train_index], X[test_index]\n",
    "    rf_y_train, rf_y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Initialize and fit RandomForestClassifier\n",
    "    rf_model = RandomForestClassifier(random_state=42)\n",
    "    rf_model.fit(rf_X_train, rf_y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    rf_y_pred = rf_model.predict(rf_X_test)\n",
    "\n",
    "    # Calculate and store metrics for this fold\n",
    "    accuracy = accuracy_score(rf_y_test, rf_y_pred)\n",
    "    precision = precision_score(rf_y_test, rf_y_pred, labels=labels, average='macro', zero_division=0)\n",
    "    recall = recall_score(rf_y_test, rf_y_pred, labels=labels, average='macro', zero_division=0)\n",
    "    f1 = f1_score(rf_y_test, rf_y_pred, labels=labels, average='macro', zero_division=0)\n",
    "    conf_matrix = confusion_matrix(rf_y_test, rf_y_pred, labels=labels)\n",
    "\n",
    "    # Store metrics for this fold\n",
    "    rf_accuracies.append(accuracy)\n",
    "    rf_precisions.append(precision)\n",
    "    rf_recalls.append(recall)\n",
    "    rf_f1s.append(f1)\n",
    "    rf_conf_matrices.append(conf_matrix)\n",
    "\n",
    "    # Collect all true and predicted labels for final confusion matrix\n",
    "    all_y_true.extend(rf_y_test)\n",
    "    all_y_pred.extend(rf_y_pred)\n",
    "\n",
    "# Calculate the final confusion matrix using all folds' predictions\n",
    "final_confusion_matrix = confusion_matrix(all_y_true, all_y_pred, labels=labels)\n",
    "\n",
    "# Optionally add functionality to calculate totals for the confusion matrix if required\n",
    "# matrix_with_totals = myutils.calculate_confusion_matrix_totals(final_confusion_matrix)\n",
    "\n",
    "# Print or return the results as needed\n",
    "print(\"Accuracies:\", rf_accuracies)\n",
    "print(\"Precisions:\", rf_precisions)\n",
    "print(\"Recalls:\", rf_recalls)\n",
    "print(\"F1 Scores:\", rf_f1s)\n",
    "print(\"Final Confusion Matrix:\\n\", final_confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Random Forest Classifier Performance\n",
      "===========================================\n",
      "Accuracy: 0.98\n",
      "Error: 0.02\n",
      "Precision: 0.98\n",
      "Recall: 0.98\n",
      "F1 Score: 0.98\n",
      "\n",
      "Confusion Matrix:\n",
      "  Delayed    0    1    2    3    4    5    Total    Recognition %\n",
      "---------  ---  ---  ---  ---  ---  ---  -------  ---------------\n",
      "        0  289   46    9   13    8    4      369            78.32\n",
      "        1   52  207   23   21    4    6      313            66.13\n",
      "        2   18   36  117   39    8    9      227            51.54\n",
      "        3   17   17   18  227   28   17      324            70.06\n",
      "        4   13   12   12   36  181   30      284            63.73\n",
      "        5   29   14   21   30   38  151      283            53.36\n"
     ]
    }
   ],
   "source": [
    "print(\"===========================================\")\n",
    "print(\"Random Forest Classifier Performance\")\n",
    "print(\"===========================================\")\n",
    "\n",
    "# Calculate and print average performance\n",
    "print(f\"Accuracy: {np.mean(rf_accuracies):.2f}\")\n",
    "print(f\"Error: {1 - np.mean(rf_accuracies):.2f}\")\n",
    "print(f\"Precision: {np.mean(rf_precisions):.2f}\")\n",
    "print(f\"Recall: {np.mean(rf_recalls):.2f}\")\n",
    "print(f\"F1 Score: {np.mean(rf_f1s):.2f}\\n\")\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "myutils.display_confusion_matrix(matrix_with_totals, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "True Labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Predicted Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "Fold 2\n",
      "True Labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Predicted Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "Fold 3\n",
      "True Labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Predicted Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "Fold 4\n",
      "True Labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Predicted Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "Fold 5\n",
      "True Labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Predicted Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "Fold 6\n",
      "True Labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Predicted Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Train Random Forest\u001b[39;00m\n\u001b[1;32m     31\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m MyRandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m rf_model\u001b[38;5;241m.\u001b[39mfit(rf_X_train, rf_y_train)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Predictions\u001b[39;00m\n\u001b[1;32m     35\u001b[0m rf_y_pred \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mpredict(rf_X_test)\n",
      "File \u001b[0;32m/home/DataScienceFinal/mysklearn/myclassifiers.py:658\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    656\u001b[0m \n\u001b[1;32m    657\u001b[0m         tree_accuracy_pairs.sort(key=lambda x: x[1], reverse=True)\n\u001b[0;32m--> 658\u001b[0m         self.trees = [pair[0] for pair in tree_accuracy_pairs[:self.m]]\n\u001b[1;32m    659\u001b[0m \n\u001b[1;32m    660\u001b[0m         print(f\"num trees: {len(self.trees)}\")\n",
      "File \u001b[0;32m/home/DataScienceFinal/mysklearn/myclassifiers.py:483\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    481\u001b[0m     self.m = m\n\u001b[1;32m    482\u001b[0m     self.f = f\n\u001b[0;32m--> 483\u001b[0m \n\u001b[1;32m    484\u001b[0m def partition_instances(self, instances, attribute):\n\u001b[1;32m    485\u001b[0m     \"\"\"Partitions instances in this format: {att val1:part1, att val2:part2}\n",
      "File \u001b[0;32m/home/DataScienceFinal/mysklearn/myclassifiers.py:584\u001b[0m, in \u001b[0;36m_build_tree\u001b[0;34m(self, X, y, depth)\u001b[0m\n\u001b[1;32m    582\u001b[0m cls_indices = [i for i, y in enumerate(y_train) if y == cls]\n\u001b[1;32m    583\u001b[0m np.random.shuffle(cls_indices)\n\u001b[0;32m--> 584\u001b[0m split_point = count // 3\n\u001b[1;32m    585\u001b[0m test_indices.extend(cls_indices[:split_point])\n\u001b[1;32m    586\u001b[0m remainder_indices.extend(cls_indices[split_point:])\n",
      "File \u001b[0;32m/home/DataScienceFinal/mysklearn/myclassifiers.py:561\u001b[0m, in \u001b[0;36m_best_split\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    559\u001b[0m # doesn't mean recurse, means change tree = ['Attribute', attribute_val] to majority vote leaf node\n\u001b[1;32m    560\u001b[0m elif len(att_partition) == 0:\n\u001b[0;32m--> 561\u001b[0m     #print(\"CASE 3\")\n\u001b[1;32m    562\u001b[0m     majority_class = myutils.get_majority_class(current_instances)\n\u001b[1;32m    563\u001b[0m     leaf_node =[\"Leaf\", majority_class, len(current_instances), prev_instances_len]\n",
      "File \u001b[0;32m/home/DataScienceFinal/mysklearn/myclassifiers.py:527\u001b[0m, in \u001b[0;36m_split\u001b[0;34m(self, X, y, feature, threshold)\u001b[0m\n\u001b[1;32m    525\u001b[0m \n\u001b[1;32m    526\u001b[0m if prev_instances_len is None:\n\u001b[0;32m--> 527\u001b[0m     prev_instances_len = len(current_instances)\n\u001b[1;32m    528\u001b[0m \n\u001b[1;32m    529\u001b[0m # basic approach (uses recursion!!):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Convert features and labels to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Perform manual stratified K-fold\n",
    "n_splits = 10\n",
    "indices = np.arange(len(y))\n",
    "unique_classes, y_counts = np.unique(y, return_counts=True)\n",
    "folds = {i: [] for i in range(n_splits)}\n",
    "\n",
    "for cls in unique_classes:\n",
    "    cls_indices = indices[y == cls]\n",
    "    np.random.shuffle(cls_indices)\n",
    "    for i, index in enumerate(cls_indices):\n",
    "        folds[i % n_splits].append(index)\n",
    "\n",
    "fold_indices = [np.array(folds[i]) for i in range(n_splits)]\n",
    "\n",
    "# Metrics storage\n",
    "rf_accuracies, rf_precisions, rf_recalls, rf_f1s = [], [], [], []\n",
    "\n",
    "# Iterate through folds\n",
    "for i in range(n_splits):\n",
    "    test_indices = fold_indices[i]\n",
    "    train_indices = np.concatenate([fold_indices[j] for j in range(n_splits) if j != i])\n",
    "\n",
    "    rf_X_train, rf_X_test = X[train_indices], X[test_indices]\n",
    "    rf_y_train, rf_y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "    # Train Random Forest\n",
    "    rf_model = MyRandomForestClassifier(n_estimators=10, max_depth=5)\n",
    "    rf_model.fit(rf_X_train, rf_y_train)\n",
    "\n",
    "    # Predictions\n",
    "    rf_y_pred = rf_model.predict(rf_X_test)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = np.sum(rf_y_test == rf_y_pred) / len(rf_y_test)\n",
    "    precision = myevaluation.binary_precision_score(rf_y_test, rf_y_pred)\n",
    "    recall = myevaluation.binary_recall_score(rf_y_test, rf_y_pred)\n",
    "    f1 = myevaluation.binary_f1_score(rf_y_test, rf_y_pred)\n",
    "\n",
    "    # Store results\n",
    "    rf_accuracies.append(accuracy)\n",
    "    rf_precisions.append(precision)\n",
    "    rf_recalls.append(recall)\n",
    "    rf_f1s.append(f1)\n",
    "\n",
    "    # Debugging outputs\n",
    "    print(f\"Fold {i + 1}\")\n",
    "    print(\"True Labels:\", rf_y_test)\n",
    "    print(\"Predicted Labels:\", rf_y_pred)\n",
    "\n",
    "# Print final metrics\n",
    "print(\"Accuracies:\", rf_accuracies)\n",
    "print(\"Precisions:\", rf_precisions)\n",
    "print(\"Recalls:\", rf_recalls)\n",
    "print(\"F1 Scores:\", rf_f1s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Random Forest Classifier Performance\n",
      "===========================================\n",
      "Accuracy: 0.97\n",
      "Error: 0.03\n",
      "Precision: 0.96\n",
      "Recall: 1.00\n",
      "F1 Score: 0.98\n",
      "\n",
      "Confusion Matrix:\n",
      "  Delayed    0    1    2    3    4    5    Total    Recognition %\n",
      "---------  ---  ---  ---  ---  ---  ---  -------  ---------------\n",
      "        0   39   10    4    5    4    4       66            59.09\n",
      "        1   12   15    9    9    2    1       48            31.25\n",
      "        2    4   10   14    3    2    4       37            37.84\n",
      "        3    6   10    1   21   10    9       57            36.84\n",
      "        4   10    2    5    9   11    8       45            24.44\n",
      "        5   11    1    5   11    8   11       47            23.40\n"
     ]
    }
   ],
   "source": [
    "print(\"===========================================\")\n",
    "print(\"Random Forest Classifier Performance\")\n",
    "print(\"===========================================\")\n",
    "\n",
    "# Calculate and print average performance\n",
    "print(f\"Accuracy: {np.mean(rf_accuracies):.2f}\")\n",
    "print(f\"Error: {1 - np.mean(rf_accuracies):.2f}\")\n",
    "print(f\"Precision: {np.mean(rf_precisions):.2f}\")\n",
    "print(f\"Recall: {np.mean(rf_recalls):.2f}\")\n",
    "print(f\"F1 Score: {np.mean(rf_f1s):.2f}\\n\")\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "myutils.display_confusion_matrix(matrix_with_totals, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
